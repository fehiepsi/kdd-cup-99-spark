{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KDD Cup 99 - PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my try with the *KDD Cup of 1999* using Python, Scikit-learn, and Spark.  \n",
    "The dataset for this data mining competition can be found [here](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task description summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the complete description of the task [here](http://kdd.ics.uci.edu/databases/kddcup99/task.html).  \n",
    "\n",
    "Software to detect network intrusions protects a computer network from unauthorized users, including perhaps insiders.  The intrusion detector learning task is to build a predictive model (i.e. a classifier) capable of distinguishing between *bad connections*, called intrusions or attacks, and *good normal connections*.  \n",
    "\n",
    "A connection is a sequence of TCP packets starting and ending at some well defined times, between which data flows to and from a source IP address to a target IP address under some well defined protocol.  Each connection is labeled as either normal, or as an attack, with exactly one specific attack type.  Each connection record consists of about 100 bytes.  \n",
    "\n",
    "Attacks fall into four main categories:  \n",
    "\n",
    "- DOS: denial-of-service, e.g. syn flood;  \n",
    "- R2L: unauthorized access from a remote machine, e.g. guessing password;  \n",
    "- U2R:  unauthorized access to local superuser (root) privileges, e.g., various ``buffer overflow'' attacks;  \n",
    "- probing: surveillance and other probing, e.g., port scanning.  \n",
    "\n",
    "It is important to note that the test data is not from the same probability distribution as the training data, and it includes specific attack types not in the training data. This makes the task more realistic. The datasets contain a total of 24 training attack types, with an additional 14 types in the test data only.    \n",
    "\n",
    "Some intrusion experts believe that most novel attacks are variants of known attacks and the \"signature\" of known attacks can be sufficient to catch novel variants. Based on this idea, we will experiment with different machine learning approaches.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by working on a reduced dataset (the 10 percent dataset provided).  \n",
    "\n",
    "There we will do some exploratory data analysis using `Pandas`. Then we will build a classifier using `Scikit-learn`. Our classifier will just classify entries into `normal` or `attack`. By doing so, we can generalise the model to new attack types.    \n",
    "\n",
    "However, in our final approach we want to use clustering and anomality detection. We want our model to be able to work well with unknown attack types and also to give an approchimation of the closest attack type. Initially we will do clustering using `Scikit-learn` again and see if we can beat our previous classifier.  \n",
    "\n",
    "Finally, we will use `Spark` to implement the clustering approach on the complete dataset containing around 5 million interactions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>494021.000000</td>\n",
       "      <td>4.940210e+05</td>\n",
       "      <td>4.940210e+05</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>47.979302</td>\n",
       "      <td>3.025610e+03</td>\n",
       "      <td>8.685324e+02</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.006433</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.034519</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.148247</td>\n",
       "      <td>0.010212</td>\n",
       "      <td>...</td>\n",
       "      <td>232.470778</td>\n",
       "      <td>188.665670</td>\n",
       "      <td>0.753780</td>\n",
       "      <td>0.030906</td>\n",
       "      <td>0.601935</td>\n",
       "      <td>0.006684</td>\n",
       "      <td>0.176754</td>\n",
       "      <td>0.176443</td>\n",
       "      <td>0.058118</td>\n",
       "      <td>0.057412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>707.746472</td>\n",
       "      <td>9.882181e+05</td>\n",
       "      <td>3.304000e+04</td>\n",
       "      <td>0.006673</td>\n",
       "      <td>0.134805</td>\n",
       "      <td>0.005510</td>\n",
       "      <td>0.782103</td>\n",
       "      <td>0.015520</td>\n",
       "      <td>0.355345</td>\n",
       "      <td>1.798326</td>\n",
       "      <td>...</td>\n",
       "      <td>64.745380</td>\n",
       "      <td>106.040437</td>\n",
       "      <td>0.410781</td>\n",
       "      <td>0.109259</td>\n",
       "      <td>0.481309</td>\n",
       "      <td>0.042133</td>\n",
       "      <td>0.380593</td>\n",
       "      <td>0.380919</td>\n",
       "      <td>0.230590</td>\n",
       "      <td>0.230140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.500000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.200000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.032000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>58329.000000</td>\n",
       "      <td>6.933756e+08</td>\n",
       "      <td>5.155468e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>884.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            duration     src_bytes     dst_bytes           land  \\\n",
       "count  494021.000000  4.940210e+05  4.940210e+05  494021.000000   \n",
       "mean       47.979302  3.025610e+03  8.685324e+02       0.000045   \n",
       "std       707.746472  9.882181e+05  3.304000e+04       0.006673   \n",
       "min         0.000000  0.000000e+00  0.000000e+00       0.000000   \n",
       "25%         0.000000  4.500000e+01  0.000000e+00       0.000000   \n",
       "50%         0.000000  5.200000e+02  0.000000e+00       0.000000   \n",
       "75%         0.000000  1.032000e+03  0.000000e+00       0.000000   \n",
       "max     58329.000000  6.933756e+08  5.155468e+06       1.000000   \n",
       "\n",
       "       wrong_fragment         urgent            hot  num_failed_logins  \\\n",
       "count   494021.000000  494021.000000  494021.000000      494021.000000   \n",
       "mean         0.006433       0.000014       0.034519           0.000152   \n",
       "std          0.134805       0.005510       0.782103           0.015520   \n",
       "min          0.000000       0.000000       0.000000           0.000000   \n",
       "25%          0.000000       0.000000       0.000000           0.000000   \n",
       "50%          0.000000       0.000000       0.000000           0.000000   \n",
       "75%          0.000000       0.000000       0.000000           0.000000   \n",
       "max          3.000000       3.000000      30.000000           5.000000   \n",
       "\n",
       "           logged_in  num_compromised            ...             \\\n",
       "count  494021.000000    494021.000000            ...              \n",
       "mean        0.148247         0.010212            ...              \n",
       "std         0.355345         1.798326            ...              \n",
       "min         0.000000         0.000000            ...              \n",
       "25%         0.000000         0.000000            ...              \n",
       "50%         0.000000         0.000000            ...              \n",
       "75%         0.000000         0.000000            ...              \n",
       "max         1.000000       884.000000            ...              \n",
       "\n",
       "       dst_host_count  dst_host_srv_count  dst_host_same_srv_rate  \\\n",
       "count   494021.000000       494021.000000           494021.000000   \n",
       "mean       232.470778          188.665670                0.753780   \n",
       "std         64.745380          106.040437                0.410781   \n",
       "min          0.000000            0.000000                0.000000   \n",
       "25%        255.000000           46.000000                0.410000   \n",
       "50%        255.000000          255.000000                1.000000   \n",
       "75%        255.000000          255.000000                1.000000   \n",
       "max        255.000000          255.000000                1.000000   \n",
       "\n",
       "       dst_host_diff_srv_rate  dst_host_same_src_port_rate  \\\n",
       "count           494021.000000                494021.000000   \n",
       "mean                 0.030906                     0.601935   \n",
       "std                  0.109259                     0.481309   \n",
       "min                  0.000000                     0.000000   \n",
       "25%                  0.000000                     0.000000   \n",
       "50%                  0.000000                     1.000000   \n",
       "75%                  0.040000                     1.000000   \n",
       "max                  1.000000                     1.000000   \n",
       "\n",
       "       dst_host_srv_diff_host_rate  dst_host_serror_rate  \\\n",
       "count                494021.000000         494021.000000   \n",
       "mean                      0.006684              0.176754   \n",
       "std                       0.042133              0.380593   \n",
       "min                       0.000000              0.000000   \n",
       "25%                       0.000000              0.000000   \n",
       "50%                       0.000000              0.000000   \n",
       "75%                       0.000000              0.000000   \n",
       "max                       1.000000              1.000000   \n",
       "\n",
       "       dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "count             494021.000000         494021.000000   \n",
       "mean                   0.176443              0.058118   \n",
       "std                    0.380919              0.230590   \n",
       "min                    0.000000              0.000000   \n",
       "25%                    0.000000              0.000000   \n",
       "50%                    0.000000              0.000000   \n",
       "75%                    0.000000              0.000000   \n",
       "max                    1.000000              1.000000   \n",
       "\n",
       "       dst_host_srv_rerror_rate  \n",
       "count             494021.000000  \n",
       "mean                   0.057412  \n",
       "std                    0.230140  \n",
       "min                    0.000000  \n",
       "25%                    0.000000  \n",
       "50%                    0.000000  \n",
       "75%                    0.000000  \n",
       "max                    1.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "from time import time\n",
    "col_names = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\",\n",
    "             \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\",\n",
    "             \"num_failed_logins\", \"logged_in\", \"num_compromised\", \"root_shell\",\n",
    "             \"su_attempted\", \"num_root\", \"num_file_creations\", \"num_shells\",\n",
    "             \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\",\n",
    "             \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\",\n",
    "             \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
    "             \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n",
    "             \"dst_host_count\", \"dst_host_srv_count\", \"dst_host_same_srv_rate\",\n",
    "             \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
    "             \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\",\n",
    "             \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\",\n",
    "             \"dst_host_srv_rerror_rate\",\"label\"]\n",
    "kdd_data_10percent = pandas.read_csv(\"data/kddcup.data_10_percent_corrected\",\n",
    "                                     header=None, names = col_names)\n",
    "kdd_data_10percent.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our data loaded into a `Pandas` data frame. In order to get familiar with our data, let's have a look at how the labels are distributed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smurf.              280790\n",
       "neptune.            107201\n",
       "normal.              97278\n",
       "back.                 2203\n",
       "satan.                1589\n",
       "ipsweep.              1247\n",
       "portsweep.            1040\n",
       "warezclient.          1020\n",
       "teardrop.              979\n",
       "pod.                   264\n",
       "nmap.                  231\n",
       "guess_passwd.           53\n",
       "buffer_overflow.        30\n",
       "land.                   21\n",
       "warezmaster.            20\n",
       "imap.                   12\n",
       "rootkit.                10\n",
       "loadmodule.              9\n",
       "ftp_write.               8\n",
       "multihop.                7\n",
       "phf.                     4\n",
       "perl.                    3\n",
       "spy.                     2\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_data_10percent['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we will use all features. We need to do something with our categorical variables. For now, we will not include them in the training features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>494021.000000</td>\n",
       "      <td>4.940210e+05</td>\n",
       "      <td>4.940210e+05</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>47.979302</td>\n",
       "      <td>3.025610e+03</td>\n",
       "      <td>8.685324e+02</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.006433</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.034519</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.148247</td>\n",
       "      <td>0.010212</td>\n",
       "      <td>...</td>\n",
       "      <td>232.470778</td>\n",
       "      <td>188.665670</td>\n",
       "      <td>0.753780</td>\n",
       "      <td>0.030906</td>\n",
       "      <td>0.601935</td>\n",
       "      <td>0.006684</td>\n",
       "      <td>0.176754</td>\n",
       "      <td>0.176443</td>\n",
       "      <td>0.058118</td>\n",
       "      <td>0.057412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>707.746472</td>\n",
       "      <td>9.882181e+05</td>\n",
       "      <td>3.304000e+04</td>\n",
       "      <td>0.006673</td>\n",
       "      <td>0.134805</td>\n",
       "      <td>0.005510</td>\n",
       "      <td>0.782103</td>\n",
       "      <td>0.015520</td>\n",
       "      <td>0.355345</td>\n",
       "      <td>1.798326</td>\n",
       "      <td>...</td>\n",
       "      <td>64.745380</td>\n",
       "      <td>106.040437</td>\n",
       "      <td>0.410781</td>\n",
       "      <td>0.109259</td>\n",
       "      <td>0.481309</td>\n",
       "      <td>0.042133</td>\n",
       "      <td>0.380593</td>\n",
       "      <td>0.380919</td>\n",
       "      <td>0.230590</td>\n",
       "      <td>0.230140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.500000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.200000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.032000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>58329.000000</td>\n",
       "      <td>6.933756e+08</td>\n",
       "      <td>5.155468e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>884.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            duration     src_bytes     dst_bytes           land  \\\n",
       "count  494021.000000  4.940210e+05  4.940210e+05  494021.000000   \n",
       "mean       47.979302  3.025610e+03  8.685324e+02       0.000045   \n",
       "std       707.746472  9.882181e+05  3.304000e+04       0.006673   \n",
       "min         0.000000  0.000000e+00  0.000000e+00       0.000000   \n",
       "25%         0.000000  4.500000e+01  0.000000e+00       0.000000   \n",
       "50%         0.000000  5.200000e+02  0.000000e+00       0.000000   \n",
       "75%         0.000000  1.032000e+03  0.000000e+00       0.000000   \n",
       "max     58329.000000  6.933756e+08  5.155468e+06       1.000000   \n",
       "\n",
       "       wrong_fragment         urgent            hot  num_failed_logins  \\\n",
       "count   494021.000000  494021.000000  494021.000000      494021.000000   \n",
       "mean         0.006433       0.000014       0.034519           0.000152   \n",
       "std          0.134805       0.005510       0.782103           0.015520   \n",
       "min          0.000000       0.000000       0.000000           0.000000   \n",
       "25%          0.000000       0.000000       0.000000           0.000000   \n",
       "50%          0.000000       0.000000       0.000000           0.000000   \n",
       "75%          0.000000       0.000000       0.000000           0.000000   \n",
       "max          3.000000       3.000000      30.000000           5.000000   \n",
       "\n",
       "           logged_in  num_compromised            ...             \\\n",
       "count  494021.000000    494021.000000            ...              \n",
       "mean        0.148247         0.010212            ...              \n",
       "std         0.355345         1.798326            ...              \n",
       "min         0.000000         0.000000            ...              \n",
       "25%         0.000000         0.000000            ...              \n",
       "50%         0.000000         0.000000            ...              \n",
       "75%         0.000000         0.000000            ...              \n",
       "max         1.000000       884.000000            ...              \n",
       "\n",
       "       dst_host_count  dst_host_srv_count  dst_host_same_srv_rate  \\\n",
       "count   494021.000000       494021.000000           494021.000000   \n",
       "mean       232.470778          188.665670                0.753780   \n",
       "std         64.745380          106.040437                0.410781   \n",
       "min          0.000000            0.000000                0.000000   \n",
       "25%        255.000000           46.000000                0.410000   \n",
       "50%        255.000000          255.000000                1.000000   \n",
       "75%        255.000000          255.000000                1.000000   \n",
       "max        255.000000          255.000000                1.000000   \n",
       "\n",
       "       dst_host_diff_srv_rate  dst_host_same_src_port_rate  \\\n",
       "count           494021.000000                494021.000000   \n",
       "mean                 0.030906                     0.601935   \n",
       "std                  0.109259                     0.481309   \n",
       "min                  0.000000                     0.000000   \n",
       "25%                  0.000000                     0.000000   \n",
       "50%                  0.000000                     1.000000   \n",
       "75%                  0.040000                     1.000000   \n",
       "max                  1.000000                     1.000000   \n",
       "\n",
       "       dst_host_srv_diff_host_rate  dst_host_serror_rate  \\\n",
       "count                494021.000000         494021.000000   \n",
       "mean                      0.006684              0.176754   \n",
       "std                       0.042133              0.380593   \n",
       "min                       0.000000              0.000000   \n",
       "25%                       0.000000              0.000000   \n",
       "50%                       0.000000              0.000000   \n",
       "75%                       0.000000              0.000000   \n",
       "max                       1.000000              1.000000   \n",
       "\n",
       "       dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "count             494021.000000         494021.000000   \n",
       "mean                   0.176443              0.058118   \n",
       "std                    0.380919              0.230590   \n",
       "min                    0.000000              0.000000   \n",
       "25%                    0.000000              0.000000   \n",
       "50%                    0.000000              0.000000   \n",
       "75%                    0.000000              0.000000   \n",
       "max                    1.000000              1.000000   \n",
       "\n",
       "       dst_host_srv_rerror_rate  \n",
       "count             494021.000000  \n",
       "mean                   0.057412  \n",
       "std                    0.230140  \n",
       "min                    0.000000  \n",
       "25%                    0.000000  \n",
       "50%                    0.000000  \n",
       "75%                    0.000000  \n",
       "max                    1.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = [\n",
    "    \"duration\", \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n",
    "    \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\", \"root_shell\",\n",
    "    \"su_attempted\", \"num_root\", \"num_file_creations\", \"num_shells\",\n",
    "    \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\", \"is_guest_login\",\n",
    "    \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\",\n",
    "    \"srv_rerror_rate\", \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n",
    "    \"dst_host_count\", \"dst_host_srv_count\", \"dst_host_same_srv_rate\",\n",
    "    \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\",\n",
    "    \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\",\n",
    "    \"dst_host_srv_rerror_rate\"]\n",
    "features = kdd_data_10percent[num_features].astype(float)\n",
    "features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned, we are going to reduce the outputs to `normal` and `attack`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attack.    396743\n",
       "normal.     97278\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "labels = kdd_data_10percent['label'].copy()\n",
    "labels[labels!='normal.'] = 'attack.'\n",
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a lot of distance-based methods here. In order to avoid some features distances dominate others, we need to scale all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>494021.000000</td>\n",
       "      <td>4.940210e+05</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000823</td>\n",
       "      <td>4.363595e-06</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.002144</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.148247</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.911650</td>\n",
       "      <td>0.739865</td>\n",
       "      <td>0.753780</td>\n",
       "      <td>0.030906</td>\n",
       "      <td>0.601935</td>\n",
       "      <td>0.006684</td>\n",
       "      <td>0.176754</td>\n",
       "      <td>0.176443</td>\n",
       "      <td>0.058118</td>\n",
       "      <td>0.057412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.012134</td>\n",
       "      <td>1.425228e-03</td>\n",
       "      <td>0.006409</td>\n",
       "      <td>0.006673</td>\n",
       "      <td>0.044935</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>0.026070</td>\n",
       "      <td>0.003104</td>\n",
       "      <td>0.355345</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253903</td>\n",
       "      <td>0.415845</td>\n",
       "      <td>0.410781</td>\n",
       "      <td>0.109259</td>\n",
       "      <td>0.481309</td>\n",
       "      <td>0.042133</td>\n",
       "      <td>0.380593</td>\n",
       "      <td>0.380919</td>\n",
       "      <td>0.230590</td>\n",
       "      <td>0.230140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.489989e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.180392</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.499542e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.488371e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            duration     src_bytes      dst_bytes           land  \\\n",
       "count  494021.000000  4.940210e+05  494021.000000  494021.000000   \n",
       "mean        0.000823  4.363595e-06       0.000168       0.000045   \n",
       "std         0.012134  1.425228e-03       0.006409       0.006673   \n",
       "min         0.000000  0.000000e+00       0.000000       0.000000   \n",
       "25%         0.000000  6.489989e-08       0.000000       0.000000   \n",
       "50%         0.000000  7.499542e-07       0.000000       0.000000   \n",
       "75%         0.000000  1.488371e-06       0.000000       0.000000   \n",
       "max         1.000000  1.000000e+00       1.000000       1.000000   \n",
       "\n",
       "       wrong_fragment         urgent            hot  num_failed_logins  \\\n",
       "count   494021.000000  494021.000000  494021.000000      494021.000000   \n",
       "mean         0.002144       0.000005       0.001151           0.000030   \n",
       "std          0.044935       0.001837       0.026070           0.003104   \n",
       "min          0.000000       0.000000       0.000000           0.000000   \n",
       "25%          0.000000       0.000000       0.000000           0.000000   \n",
       "50%          0.000000       0.000000       0.000000           0.000000   \n",
       "75%          0.000000       0.000000       0.000000           0.000000   \n",
       "max          1.000000       1.000000       1.000000           1.000000   \n",
       "\n",
       "           logged_in  num_compromised            ...             \\\n",
       "count  494021.000000    494021.000000            ...              \n",
       "mean        0.148247         0.000012            ...              \n",
       "std         0.355345         0.002034            ...              \n",
       "min         0.000000         0.000000            ...              \n",
       "25%         0.000000         0.000000            ...              \n",
       "50%         0.000000         0.000000            ...              \n",
       "75%         0.000000         0.000000            ...              \n",
       "max         1.000000         1.000000            ...              \n",
       "\n",
       "       dst_host_count  dst_host_srv_count  dst_host_same_srv_rate  \\\n",
       "count   494021.000000       494021.000000           494021.000000   \n",
       "mean         0.911650            0.739865                0.753780   \n",
       "std          0.253903            0.415845                0.410781   \n",
       "min          0.000000            0.000000                0.000000   \n",
       "25%          1.000000            0.180392                0.410000   \n",
       "50%          1.000000            1.000000                1.000000   \n",
       "75%          1.000000            1.000000                1.000000   \n",
       "max          1.000000            1.000000                1.000000   \n",
       "\n",
       "       dst_host_diff_srv_rate  dst_host_same_src_port_rate  \\\n",
       "count           494021.000000                494021.000000   \n",
       "mean                 0.030906                     0.601935   \n",
       "std                  0.109259                     0.481309   \n",
       "min                  0.000000                     0.000000   \n",
       "25%                  0.000000                     0.000000   \n",
       "50%                  0.000000                     1.000000   \n",
       "75%                  0.040000                     1.000000   \n",
       "max                  1.000000                     1.000000   \n",
       "\n",
       "       dst_host_srv_diff_host_rate  dst_host_serror_rate  \\\n",
       "count                494021.000000         494021.000000   \n",
       "mean                      0.006684              0.176754   \n",
       "std                       0.042133              0.380593   \n",
       "min                       0.000000              0.000000   \n",
       "25%                       0.000000              0.000000   \n",
       "50%                       0.000000              0.000000   \n",
       "75%                       0.000000              0.000000   \n",
       "max                       1.000000              1.000000   \n",
       "\n",
       "       dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "count             494021.000000         494021.000000   \n",
       "mean                   0.176443              0.058118   \n",
       "std                    0.380919              0.230590   \n",
       "min                    0.000000              0.000000   \n",
       "25%                    0.000000              0.000000   \n",
       "50%                    0.000000              0.000000   \n",
       "75%                    0.000000              0.000000   \n",
       "max                    1.000000              1.000000   \n",
       "\n",
       "       dst_host_srv_rerror_rate  \n",
       "count             494021.000000  \n",
       "mean                   0.057412  \n",
       "std                    0.230140  \n",
       "min                    0.000000  \n",
       "25%                    0.000000  \n",
       "50%                    0.000000  \n",
       "75%                    0.000000  \n",
       "max                    1.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "features[features.columns] = MinMaxScaler().fit_transform(features)\n",
    "features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising data using Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using Principal Component Analysis, we can reduce the dimensionality of our data and plot it into a two-dimensional space. The PCA will capture those dimensions with the maximum variance, reducing the information loss.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the idea that new attack types will be similar to known types, let's start by trying a k-nearest neighbours classifier. We must to avoid brute force comparisons in the Nxd space at all costs. Being N the number of samples in our data more than 400K, and d the number of features 38, we will end up with an unfeasible modeling process. For this reason we pass `algorithm = 'ball_tree'`. For more on kNN performance, check [here](http://scikit-learn.org/stable/modules/neighbors.html#choice-of-nearest-neighbors-algorithm).      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained in 1041.908 seconds\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors = 5, algorithm = 'ball_tree',\n",
    "                           leaf_size=500)\n",
    "t0 = time()\n",
    "clf.fit(features, labels)\n",
    "tt = time()-t0\n",
    "print(\"Classifier trained in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the classifier with the testing data. First we need to load the labeled test data. We wil also sample 10 percent of the entries. For that. we will take advantage of the `train_test_split` function in `sklearn`.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smurf.              164091\n",
       "normal.              60593\n",
       "neptune.             58001\n",
       "snmpgetattack.        7741\n",
       "mailbomb.             5000\n",
       "guess_passwd.         4367\n",
       "snmpguess.            2406\n",
       "satan.                1633\n",
       "warezmaster.          1602\n",
       "back.                 1098\n",
       "mscan.                1053\n",
       "apache2.               794\n",
       "processtable.          759\n",
       "saint.                 736\n",
       "portsweep.             354\n",
       "ipsweep.               306\n",
       "httptunnel.            158\n",
       "pod.                    87\n",
       "nmap.                   84\n",
       "buffer_overflow.        22\n",
       "multihop.               18\n",
       "named.                  17\n",
       "sendmail.               17\n",
       "ps.                     16\n",
       "xterm.                  13\n",
       "rootkit.                13\n",
       "teardrop.               12\n",
       "land.                    9\n",
       "xlock.                   9\n",
       "xsnoop.                  4\n",
       "ftp_write.               3\n",
       "loadmodule.              2\n",
       "sqlattack.               2\n",
       "worm.                    2\n",
       "udpstorm.                2\n",
       "perl.                    2\n",
       "phf.                     2\n",
       "imap.                    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_data_corrected = pandas.read_csv(\"data/corrected\", header=None,\n",
    "                                     names = col_names)\n",
    "kdd_data_corrected['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have new attack labels. In any case, we will convert all of the to the `attack.` label.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attack.    250436\n",
       "normal.     60593\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_data_corrected['label'].apply(lambda x: 'attack.' if x != 'normal.' else x)\n",
    "kdd_data_corrected['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we select features and scale.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "kdd_data_corrected[num_features] = kdd_data_corrected[\n",
    "    num_features].astype(float)\n",
    "kdd_data_corrected[num_features] = MinMaxScaler().fit_transform(\n",
    "    kdd_data_corrected[num_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can sample the 10 percent of the test data (after we scale it). Although we also get training data, we don't need it in this case.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = train_test_split(\n",
    "    kdd_data_corrected[num_features], \n",
    "    kdd_data_corrected['label'], \n",
    "    test_size=0.1, \n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, do predictions using our classifier and the test data. kNN classifiers are slow compared to other methods due to all the comparisons required in order to make predictions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted in 377.678 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "pred = clf.predict(features_test)\n",
    "tt = time() - t0\n",
    "print(\"Predicted in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That took a lot of time. Actually, the more training data we use with a kNN classifier, the slower it gets to predict. It needs to compare the new data with all the points. **Definitively we want some centroid-based classifier if we plan to use it in real-time detection.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, calculate the R squared value using the test labels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared is 0.92.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(pred, labels_test)\n",
    "print(\"R squared is {}.\".format(round(acc,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So finally, let's try our anomaly detection approach in the reduced dataset. We will start by doing **k-means clustering**. Once we have the cluster centers, we will use them to determine the labels of the test data (unlabeled).  \n",
    "\n",
    "Based on the assumption that new attack types will resemble old type, we will be able to detect those. Moreover, anything that falls too far from any cluster, will be considered anomalous and therefore a possible attack.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k = 30\n",
    "km = KMeans(n_clusters = k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustered in 73.708 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "km.fit(features)\n",
    "tt = time()-t0\n",
    "print(\"Clustered in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check cluster sizes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     280630\n",
       "1      48557\n",
       "16     38321\n",
       "8      25557\n",
       "3      20528\n",
       "5      18979\n",
       "7      10600\n",
       "27      8666\n",
       "4       4280\n",
       "6       4213\n",
       "13      3702\n",
       "26      3380\n",
       "9       3279\n",
       "10      3154\n",
       "2       2965\n",
       "14      2911\n",
       "20      1655\n",
       "25      1599\n",
       "21      1343\n",
       "15      1243\n",
       "12      1230\n",
       "17      1200\n",
       "19      1194\n",
       "11      1020\n",
       "18       971\n",
       "29       934\n",
       "23       783\n",
       "22       673\n",
       "28       279\n",
       "24       175\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas.Series(km.labels_).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get labels for each cluster. Here, we go back to use the complete set of labels.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = kdd_data_10percent['label']\n",
    "label_names = list(map(lambda x: pandas.Series(\n",
    "        [labels[i] for i in range(len(km.labels_)) if km.labels_[i]==x]),\n",
    "                   range(k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print labels for each cluster.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 labels:\n",
      "smurf.     280599\n",
      "normal.        31\n",
      "dtype: int64\n",
      "\n",
      "Cluster 1 labels:\n",
      "neptune.      48553\n",
      "portsweep.        4\n",
      "dtype: int64\n",
      "\n",
      "Cluster 2 labels:\n",
      "normal.    2885\n",
      "back.        80\n",
      "dtype: int64\n",
      "\n",
      "Cluster 3 labels:\n",
      "neptune.      20456\n",
      "portsweep.       58\n",
      "satan.           10\n",
      "normal.           4\n",
      "dtype: int64\n",
      "\n",
      "Cluster 4 labels:\n",
      "normal.       4245\n",
      "satan.          26\n",
      "portsweep.       9\n",
      "dtype: int64\n",
      "\n",
      "Cluster 5 labels:\n",
      "normal.    18979\n",
      "dtype: int64\n",
      "\n",
      "Cluster 6 labels:\n",
      "normal.          4152\n",
      "guess_passwd.      49\n",
      "back.               7\n",
      "ipsweep.            3\n",
      "portsweep.          1\n",
      "neptune.            1\n",
      "dtype: int64\n",
      "\n",
      "Cluster 7 labels:\n",
      "normal.    10594\n",
      "smurf.         6\n",
      "dtype: int64\n",
      "\n",
      "Cluster 8 labels:\n",
      "normal.    23958\n",
      "back.       1596\n",
      "phf.           3\n",
      "dtype: int64\n",
      "\n",
      "Cluster 9 labels:\n",
      "normal.       3106\n",
      "pod.            88\n",
      "smurf.          41\n",
      "satan.          18\n",
      "teardrop.        7\n",
      "ipsweep.         6\n",
      "nmap.            5\n",
      "rootkit.         3\n",
      "portsweep.       2\n",
      "spy.             1\n",
      "neptune.         1\n",
      "land.            1\n",
      "dtype: int64\n",
      "\n",
      "Cluster 10 labels:\n",
      "normal.             2705\n",
      "back.                439\n",
      "buffer_overflow.       7\n",
      "loadmodule.            2\n",
      "guess_passwd.          1\n",
      "dtype: int64\n",
      "\n",
      "Cluster 11 labels:\n",
      "portsweep.    934\n",
      "ipsweep.       83\n",
      "normal.         3\n",
      "dtype: int64\n",
      "\n",
      "Cluster 12 labels:\n",
      "satan.        1222\n",
      "portsweep.       8\n",
      "dtype: int64\n",
      "\n",
      "Cluster 13 labels:\n",
      "normal.         3649\n",
      "back.             51\n",
      "warezclient.       1\n",
      "phf.               1\n",
      "dtype: int64\n",
      "\n",
      "Cluster 14 labels:\n",
      "normal.    2910\n",
      "back.         1\n",
      "dtype: int64\n",
      "\n",
      "Cluster 15 labels:\n",
      "ipsweep.     813\n",
      "normal.      163\n",
      "pod.         153\n",
      "nmap.         99\n",
      "land.         14\n",
      "multihop.      1\n",
      "dtype: int64\n",
      "\n",
      "Cluster 16 labels:\n",
      "neptune.         38188\n",
      "nmap.              103\n",
      "portsweep.          19\n",
      "normal.              6\n",
      "imap.                2\n",
      "land.                2\n",
      "guess_passwd.        1\n",
      "dtype: int64\n",
      "\n",
      "Cluster 17 labels:\n",
      "normal.    1200\n",
      "dtype: int64\n",
      "\n",
      "Cluster 18 labels:\n",
      "teardrop.    971\n",
      "dtype: int64\n",
      "\n",
      "Cluster 19 labels:\n",
      "warezclient.        661\n",
      "normal.             492\n",
      "buffer_overflow.     22\n",
      "ftp_write.            6\n",
      "loadmodule.           5\n",
      "back.                 5\n",
      "multihop.             2\n",
      "rootkit.              1\n",
      "dtype: int64\n",
      "\n",
      "Cluster 20 labels:\n",
      "normal.          1336\n",
      "smurf.            144\n",
      "ipsweep.           77\n",
      "nmap.              24\n",
      "satan.             20\n",
      "warezmaster.       18\n",
      "pod.               15\n",
      "imap.               9\n",
      "land.               3\n",
      "rootkit.            2\n",
      "multihop.           2\n",
      "portsweep.          1\n",
      "ftp_write.          1\n",
      "loadmodule.         1\n",
      "neptune.            1\n",
      "guess_passwd.       1\n",
      "dtype: int64\n",
      "\n",
      "Cluster 21 labels:\n",
      "normal.       1230\n",
      "satan.         111\n",
      "portsweep.       1\n",
      "teardrop.        1\n",
      "dtype: int64\n",
      "\n",
      "Cluster 22 labels:\n",
      "normal.         363\n",
      "warezclient.    306\n",
      "multihop.         2\n",
      "warezmaster.      2\n",
      "dtype: int64\n",
      "\n",
      "Cluster 23 labels:\n",
      "normal.             768\n",
      "satan.                9\n",
      "warezclient.          4\n",
      "buffer_overflow.      1\n",
      "land.                 1\n",
      "dtype: int64\n",
      "\n",
      "Cluster 24 labels:\n",
      "satan.        172\n",
      "portsweep.      3\n",
      "dtype: int64\n",
      "\n",
      "Cluster 25 labels:\n",
      "normal.    1599\n",
      "dtype: int64\n",
      "\n",
      "Cluster 26 labels:\n",
      "normal.         3319\n",
      "warezclient.      48\n",
      "rootkit.           4\n",
      "perl.              3\n",
      "ipsweep.           2\n",
      "spy.               1\n",
      "loadmodule.        1\n",
      "imap.              1\n",
      "satan.             1\n",
      "dtype: int64\n",
      "\n",
      "Cluster 27 labels:\n",
      "normal.    8642\n",
      "back.        24\n",
      "dtype: int64\n",
      "\n",
      "Cluster 28 labels:\n",
      "ipsweep.         263\n",
      "pod.               8\n",
      "normal.            6\n",
      "ftp_write.         1\n",
      "guess_passwd.      1\n",
      "dtype: int64\n",
      "\n",
      "Cluster 29 labels:\n",
      "normal.     933\n",
      "neptune.      1\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(k):\n",
    "    print(\"Cluster {} labels:\".format(i))\n",
    "    print(label_names[i].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster description  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, in most clusters, there is a dominant label. It would be interesting to go cluster by cluster and analyise mayority labels, or how labels are split between different clusters (some with more dominance than others). All that would help us understand each type of attack! This is also a benefit of using a clustering-based approach.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO:\n",
    "- Get dominant labels\n",
    "- Analyse cluster centers, specially for heterogeneous clusters containing `normal`. This will discover conflictive interactions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now predict using our test data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned clusters in 0.482 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "pred = km.predict(kdd_data_corrected[num_features])\n",
    "tt = time() - t0\n",
    "print(\"Assigned clusters in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the assignment process is much faster than the prediction process with our kNN. But we still need to assign labels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: get mayority label for each cluster assignment (we have labels from the previous step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: check these labels with those in the corrected test data in order to calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the complete dataset with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script [KDDCup99.py](KDDCup99.py) runds through a series of steps to perform k-means clustering over the complete dataset using `PySpark` with different K values in order to find the best one.  \n",
    "\n",
    "The clustering results are stored in a `CSV` file. This file is very convenient for visualisation purposes. It would be very hard to cluster and visualise results of the complete dataset using `Scikit-learn`.  \n",
    "\n",
    "The following chart depicts the **first two pincipal components** for the clustering results.  \n",
    "\n",
    "![](clusters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we have up to 24 different labels in our complete dataset. However we have generated up to 80 different clusters. As a result of this, some of the clusters appear very close in the first principal component. This is due to the variability of interactions for a given type of attack (or label).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: follow the same approach for label assignment in the test data as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to show how we use `Spark` to do k-means clustering in our dataset, let's perform here a single clustering run with the complete dataset, for a K value of 80 that showed to be particulary good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some imports we will use\n",
    "from collections import OrderedDict\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load the data, using the complete dataset file stored in NFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_file = \"data/kddcup.data.corrected\"\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a warm up, let's count the number of interactions by label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting all different labels\n",
      "smurf. 2807886\n",
      "neptune. 1072017\n",
      "normal. 972781\n",
      "satan. 15892\n",
      "ipsweep. 12481\n",
      "portsweep. 10413\n",
      "nmap. 2316\n",
      "back. 2203\n",
      "warezclient. 1020\n",
      "teardrop. 979\n",
      "pod. 264\n",
      "guess_passwd. 53\n",
      "buffer_overflow. 30\n",
      "land. 21\n",
      "warezmaster. 20\n",
      "imap. 12\n",
      "rootkit. 10\n",
      "loadmodule. 9\n",
      "ftp_write. 8\n",
      "multihop. 7\n",
      "phf. 4\n",
      "perl. 3\n",
      "spy. 2\n",
      "Counted in 8.193 seconds\n"
     ]
    }
   ],
   "source": [
    "# count by all different labels and print them decreasingly\n",
    "print(\"Counting all different labels\")\n",
    "labels = raw_data.map(lambda line: line.strip().split(\",\")[-1])\n",
    "\n",
    "t0 = time()\n",
    "label_counts = labels.countByValue()\n",
    "tt = time()-t0\n",
    "\n",
    "sorted_labels = OrderedDict(sorted(label_counts.items(), key=lambda t: t[1],\n",
    "                                   reverse=True))\n",
    "for label, count in sorted_labels.items():\n",
    "    print(label, count)\n",
    "    \n",
    "print(\"Counted in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare the data for clustering input. The data contains non-numeric features, and we want to exclude them since k-means works just with numeric features. These are the first three and the last column in each data row that is the label.  \n",
    "In order to do that, we define a function that we apply to the *RDD* as a `Spark` **transformation** by using `map`. Remember that we can apply as many transofmrations as we want without making `Spark` start any processing. Is is when we trigger an action when all the transformations are applied.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_interaction(line):\n",
    "    \"\"\"\n",
    "    Parses a network data interaction.\n",
    "    \"\"\"\n",
    "    line_split = line.split(\",\")\n",
    "    clean_line_split = [line_split[0]]+line_split[4:-1]\n",
    "    return (line_split[-1], np.array([float(x) for x in clean_line_split]))\n",
    "\n",
    "parsed_data = raw_data.map(parse_interaction)\n",
    "parsed_data_values = parsed_data.values().cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we have used `cache` in order to keep the results at hand once they are calculated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also standardise our data as we have done so far when performing distance-based clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data standardized in 41.894 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.feature import StandardScaler\n",
    "standardizer = StandardScaler(True, True)\n",
    "t0 = time()\n",
    "standardizer_model = standardizer.fit(parsed_data_values)\n",
    "tt = time() - t0\n",
    "standardized_data_values = standardizer_model.transform(parsed_data_values)\n",
    "print(\"Data standardized in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform k-means clustering.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/mllib/clustering.py:347: UserWarning: The param `runs` has no effect since Spark 2.0.0.\n",
      "  warnings.warn(\"The param `runs` has no effect since Spark 2.0.0.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Py4JJavaError: An error occurred while calling o221.trainKMeansModel.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 11.0 failed 1 times, most recent failure: Lost task 3.0 in stage 11.0 (TID 203, localhost): java.io.IOException: java.util.NoSuchElementException\n",
       "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1283)\n",
       "\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:174)\n",
       "\tat org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:65)\n",
       "\tat org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:65)\n",
       "\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:89)\n",
       "\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n",
       "\tat org.apache.spark.mllib.clustering.KMeans$$anonfun$13.apply(KMeans.scala:288)\n",
       "\tat org.apache.spark.mllib.clustering.KMeans$$anonfun$13.apply(KMeans.scala:287)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "Caused by: java.util.NoSuchElementException\n",
       "\tat org.apache.spark.util.collection.PrimitiveVector$$anon$1.next(PrimitiveVector.scala:58)\n",
       "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:697)\n",
       "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:30)\n",
       "\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1$$anonfun$2.apply(TorrentBroadcast.scala:178)\n",
       "\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1$$anonfun$2.apply(TorrentBroadcast.scala:178)\n",
       "\tat scala.Option.map(Option.scala:146)\n",
       "\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:178)\n",
       "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1276)\n",
       "\t... 19 more\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\n",
       "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n",
       "\tat scala.Option.foreach(Option.scala:257)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1873)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1886)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1899)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1913)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n",
       "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:911)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:745)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:744)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:744)\n",
       "\tat org.apache.spark.mllib.clustering.KMeans.runAlgorithm(KMeans.scala:310)\n",
       "\tat org.apache.spark.mllib.clustering.KMeans.run(KMeans.scala:219)\n",
       "\tat org.apache.spark.mllib.clustering.KMeans.run(KMeans.scala:201)\n",
       "\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainKMeansModel(PythonMLLibAPI.scala:367)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:280)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "Caused by: java.io.IOException: java.util.NoSuchElementException\n",
       "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1283)\n",
       "\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:174)\n",
       "\tat org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:65)\n",
       "\tat org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:65)\n",
       "\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:89)\n",
       "\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n",
       "\tat org.apache.spark.mllib.clustering.KMeans$$anonfun$13.apply(KMeans.scala:288)\n",
       "\tat org.apache.spark.mllib.clustering.KMeans$$anonfun$13.apply(KMeans.scala:287)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\t... 1 more\n",
       "Caused by: java.util.NoSuchElementException\n",
       "\tat org.apache.spark.util.collection.PrimitiveVector$$anon$1.next(PrimitiveVector.scala:58)\n",
       "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:697)\n",
       "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:30)\n",
       "\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1$$anonfun$2.apply(TorrentBroadcast.scala:178)\n",
       "\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1$$anonfun$2.apply(TorrentBroadcast.scala:178)\n",
       "\tat scala.Option.map(Option.scala:146)\n",
       "\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:178)\n",
       "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1276)\n",
       "\t... 19 more\n",
       "\n",
       "(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling o221.trainKMeansModel.\\n', JavaObject id=o231), <traceback object at 0x7f97d0ec4988>)\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:158)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:158)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:157)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:280)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.clustering import KMeans\n",
    "t0 = time()\n",
    "clusters = KMeans.train(standardized_data_values, 80, \n",
    "                        maxIterations=10, runs=5, \n",
    "                        initializationMode=\"random\")\n",
    "tt = time() - t0\n",
    "print(\"Data clustered in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our clusters, we can use them to label test data and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
